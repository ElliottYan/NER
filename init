import numpy as np
import os
import pdb
import timex


def extract_timex():
    f1 = open(os.getcwd() + "/origin_data/train.txt","r")
    f2 = open(os.getcwd() + "/origin_data/test.txt",'r')
    fin = f1
    f3 = open(os.getcwd() + "/origin_data/train_time.txt","w")
    f4 = open(os.getcwd() + "/origin_data/test_time.txt","w")
    fo = f3
    finish = 0
    count = 0
    while(True):
        content = fin.readline()
        if content == "" and finish == 0:
            finish = 1
            fin = f2
            fo = f4
        elif content == "" and finish == 1:
            break
        content = content.split()
        new_cont = " ".join(content[5:])
        new_content = timex.tag(new_cont)
        # pdb.set_trace()
        # new_content = timex.tag(str(content[5:]))
        fo.write(" ".join(content[:5] + new_content.split()) + "\n")
        count += 1
        if(count%500 == 0):
            print("Now is in " + str(count) + " iteration!")


def init():
    fix_len = 60
    vec, word_id = read_in_word2vec()
    relation_id = read_in_relations()
    train_mention = read_in(word_id, relation_id, fix_len, train_test="train")
    test_mention = read_in(word_id, relation_id, fix_len, train_test="test")
    separate_save_in(train_mention, len(relation_id), train_test = "train")
    separate_save_in(test_mention, len(relation_id), train_test="test", vec = vec)


def pos_embed(x, lower, upper, fix_len):
    return max(lower, min(x + fix_len, upper))


def read_in_word2vec():
    print("Now reading Word_embeddings!")
    vec = []
    word_id = {}
    with open(os.getcwd()+"/origin_data/vec.txt") as f:
        f.readline()
        count = 0
        while True:
            content = f.readline()
            if content == "":
                break
            content = content.strip().split()
            word_id[content[0]] = count
            count += 1
            content = [float(c) for c in content[1:]]
            vec.append(content)
    dim = 50
    word_id["BLANK"] = len(word_id)
    vec.append(np.random.normal(size=dim, loc=0, scale=0.05))
    word_id["UNK"] = len(word_id)
    vec.append(np.random.normal(size=dim, loc=0, scale=0.05))
    vec = np.array(vec, dtype = np.float32)
    # pdb.set_trace()
    return (vec,word_id)
#   there were two random vectors which i think is unecessary.


def read_in_relations():
    print('Now reading in Relationship ids!')
    relation_id = {}
    with open(os.getcwd() + "/origin_data/relation2id.txt") as f:
        while True:
            content = f.readline()
            if content == "":
                break
            content = content.strip().split()
            relation_id[content[0]] = int(content[1])
    return relation_id


def read_in_data(func):
    def call(*args, train_test="train"):
        if train_test == "train":
            f = open(os.getcwd() + "/origin_data/train.txt")
        elif train_test == "test":
            f = open(os.getcwd() + "/origin_data/test.txt")
        else:
            print("Wrong argument in calling read_in functions!")
            return
        print("Now is time for reading the " + train_test + " data!")
        results = func(*args, f)
        f.close()
        return results
    return call


@read_in_data
def read_in(word_id, relation_id, fix_len, f):
    mentions = {}
    n_relation = len(relation_id)
    while True:
        content = f.readline()
        if content == "":
            break
        content = content.strip().split()
        # in this case, the entity pair comes in with same order
        en1 = content[2]
        en2 = content[3]
        tup = (en1, en2)
        relation = 0
        if content[4] not in relation_id:
            relation = relation_id["NA"]
        else:
            relation = relation_id[content[4]]

        # every tuple is a mentions-bag level object

        # Data Structure
        #   mentions :(tuple) -> relation -> embeddings

        # construct mentions-bag if not exist

        # if no such tuple appeared
        if tup not in mentions:
            mentions[tup] = {}

        # if no such relation appeared
        if relation not in mentions[tup].keys():
            mentions[tup][relation] = []

        sentence = content[5:-1]

        # have to write two try-catch block for both entity position.

        # still need to solve this problem
        # en1 , en2 format may not be like its mentions in sentence
        en1pos = en2pos = 0
        try:
            en1pos = sentence.index(en1)
        except ValueError:
            pass
        try:
            en2pos = sentence.index(en2)
        except ValueError:
            pass

        # construct the mentions in each bag
        # with [[word, rel_pos1, rel_pos2],....]
        # first tuple is [relation, pos1, pos2], used for PCNN of sentence
        output = [[relation, en1pos, en2pos],]
        for i in range(min(fix_len, len(sentence))):
            rel_pos1 = pos_embed(i - en1pos, 0, fix_len * 2, fix_len)
            rel_pos2 = pos_embed(i - en2pos, 0, fix_len * 2, fix_len)
            word = sentence[i]
            if word not in word_id:
                word = "UNK"
            word = word_id[word]
            output.append([word, rel_pos1, rel_pos2])

        mentions[tup][relation].append(output)

    return mentions


def save(func):
    def call(*args, train_test="train", vec=None):
        sentences, anses, targets = func(*args)
        msg = "Saving all" + train_test + "data!"
        print(msg)
        np.save("./data/"+ train_test + "_sen.npy", sentences)
        np.save("./data/" + train_test + "_ans.npy", anses)
        np.save("./data/" + train_test + "_target.npy", targets)
        np.save("./data/vec.npy", vec)
    return call


@save
def separate_save_in(sentence_bag, n_relation):
    sentence_list = []
    ans_list = []
    targets = []
    for pair,bag in sentence_bag.items():
        for relation, mentions in bag.items():
            for m in mentions:
                # m here denotes a sentence
                sentence_list.append(np.array(m))
            ans_list.append(relation)
        targets.append(pair)
    sentence_list = np.array(sentence_list)
    ans_list = np.array(ans_list)
    targets = np.array(targets)
    pdb.set_trace()
    return sentence_list, ans_list, targets


# init()
extract_timex()